. Required

connection

Type: KafkaConnectionConfig (your existing type)

What: Holds broker/SSL/etc. info and builds the actual Kafka consumer.

Use: Internally call something like connection.build_consumer(...).

topics

Type: list[str]

What: Kafka topics to subscribe to.

group_id

Type: str

What: Consumer group ID used for offset tracking in Kafka.

message_processor

Type: Callable[[MessageContext], None] (or similar)

What: Your ETL logic for one message.

Contract:

Does the actual work (parse, transform, DB/API calls, etc.).

Raises an exception on failure (framework handles it).

B. Optional (prod-ready, minimal)

worker_threads

Type: int

Default: e.g. 20

Meaning:

Max number of parallel worker threads.

Max number of messages processed concurrently.

Internal rule:

Batch size = worker_threads (not exposed separately).

Guarantee:

At most worker_threads messages are polled and in-flight at any point.

auto_offset_reset

Type: str â€” "latest" | "earliest"

Default: "latest"

Meaning: Where to start if no committed offset exists for this group.

additional_consumer_config

Type: dict[str, Any]

Default: {}

Meaning: Extra Kafka client config (security, tuning) if/when youâ€™re allowed to use it.

poll_timeout_seconds

Type: float

Default: 1.0

Meaning: How long to wait for up to worker_threads messages in each poll.

max_retries_per_message

Type: int

Default: 0

Meaning: Number of retries for a single message inside a worker thread (for transient errors).

0 â†’ one attempt only; >0 â†’ retry that many times before marking failure.

retry_backoff_ms

Type: int

Default: 1000 (1s)

Meaning: Sleep between retries for one message.

failure_handler

Type: Optional[Callable[[MessageContext, Exception], None]]

Default: None

Meaning:

Callback to send failed messages to a DLQ / failed_events table / file.

Called after all retries for that message are exhausted.

failure_mode

Type: str

Default: "capture_and_continue"

Allowed: "capture_and_continue", "stop_on_error"

Meaning:

"capture_and_continue": on failure â†’ call failure_handler, donâ€™t fail the batch, continue and commit.

"stop_on_error": on failure â†’ call failure_handler, then trigger shutdown (no more polls).

controller

Type: Optional[StreamController]

Default: None (create one internally)

Meaning: Graceful shutdown + signal handling (SIGINT/SIGTERM).

If provided, use it; otherwise construct your own and call register_signal_handlers().

shutdown_max_wait_seconds

Type: int

Default: 60

Meaning:

On shutdown: stop polling new batches, wait up to this many seconds for the current batchâ€™s threads to finish.

If exceeded, stop waiting; batch remains partially processed and will be replayed (at-least-once).

log_summary_interval

Type: int (number of messages)

Default: 1000

Meaning:

After every N processed messages, log a summary (processed/failed counts, avg times, etc.).

<= 0 â†’ disable periodic summary logging.

log_message_details

Type: bool

Default: False

Meaning:

If True: log per-message metadata (topic/partition/offsetâ€¦) â€” useful in dev.

If False: only error logs + summaries.

dev_mode

Type: bool

Default: False

Meaning:

If True: never commit offsets â†’ you can replay data for this group.

Use with a dev-only group_id to avoid touching prod offsets.

2ï¸âƒ£ New Design (Behaviour) â€” High-level

This is the design your framework should implement using those params.

2.1 Core model

One consumer loop thread, plus a ThreadPoolExecutor for parallel work.

No custom queue between consumer and workers (v1).

Batch size is fixed to worker_threads.

At any time:

Max messages polled and in-flight = worker_threads.

Max messages processed concurrently = worker_threads.

This mirrors Talendâ€™s â€œmax parallel executionsâ€, but without Talendâ€™s hidden internal queue complexity.

2.2 Message flow per loop iteration

For each iteration of the main loop (while not controller.should_stop()):

Poll from Kafka

Call consume(num_messages=worker_threads, timeout=poll_timeout_seconds).

This returns up to worker_threads messages (possibly fewer).

Create a per-message context

For each msg, build a MessageContext (your own class/dict) containing at least:

topic

partition

offset

key

value

timestamp

maybe the raw Kafka record.

Submit to thread pool

Use a single shared ThreadPoolExecutor(max_workers=worker_threads).

For each message, submit a wrapper function:

Handles:

retries (up to max_retries_per_message, with retry_backoff_ms delay),

calls message_processor(ctx),

catches final exceptions,

calls failure_handler(ctx, exc) if configured,

obeys failure_mode:

"capture_and_continue" â†’ swallow the exception (from Kafkaâ€™s POV this msg is â€œhandledâ€).

"stop_on_error" â†’ signal shutdown via controller.

Wait for batch to finish

Collect futures for all submitted tasks for this batch.

wait(futures, return_when=ALL_COMPLETED) â€” this is where the main thread blocks until:

All messages succeed, or

Failures are captured by failure_handler.

ðŸ‘‰ At this point, for this batch: nothing is â€œhalfwayâ€ â€” each message is either processed or recorded as failed.

Commit offsets (if not dev)

If dev_mode == False:

Call consumer.commit(asynchronous=False) once for this batch.

Offsets are not committed until all messages in the batch are handled.

Check shutdown & loop

Check controller.should_stop() again:

If False â†’ go back to step 1 (poll next batch).

If True â†’ exit loop, close consumer, shutdown executor.



2.5 Logging design (efficient)

Create one shared logger per component / module:

E.g. at module level: logger = logging.getLogger("kafka_consumer").

Do not create a new logger inside message_processor or per message.

Use:

log_summary_interval

Keep counters in memory (processed, failed, total time).

Every N messages, log a single summary line.

log_message_details

If False: only log errors + summary, not every message.

If True: for each message you may log basic metadata (topic/partition/offset), but avoid payload spam in prod.

On failure:

Log one error per failed message (unless too noisy), including:

topic, partition, offset

error type and message.