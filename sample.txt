Just to be super crisp:

Inner pipeline / job (run_job / whatever you call)

Owns all business/data errors.

If it knows how to handle them (reject table, logs, etc.), it should catch and NOT raise.

From the consumer’s POV: if it returns, the message is “done” (even if job treated it as reject).

Kafka consumer worker wrapper

Calls your message_processor(ctx) (which calls run_job).

If message_processor raises:

That means the job did not handle it → treat as unexpected/system error.

Common behavior in wrapper:

Log error with topic/partition/offset.

Call controller.request_stop() so main loop stops after this batch.

Do not re-raise in a way that crashes the whole process (just mark internally).

Main loop

For each batch:

Submit messages to pool.

Wait for all tasks.

If no unhandled errors in this batch → commit offsets.

If any unhandled error → do NOT commit this batch, then exit loop (shutdown).

On restart, Kafka will replay this last batch.

So:

All data/record-level error handling lives inside your job.
Consumer only cares about truly unhandled exceptions and uses them as a reason to stop + not commit that batch.

No failure handler. No per-message retries.

2️⃣ Final param list (post-cleanup, no retries/failure_handler)
Required

connection

KafkaConnectionConfig

Builds the Kafka consumer.

topics

list[str]

Topics to subscribe to.

group_id

str

Consumer group ID for offsets.

message_processor

Callable[[MessageContext], None]

Calls your job/pipeline with the message.

Contract:

Handles all expected errors internally.

Only raises on truly unexpected/system issues.

Optional

worker_threads

int, default e.g. 20

Size of thread pool.

Batch size = worker_threads (fixed).

Max in-flight and parallel messages = worker_threads.

auto_offset_reset

"latest" | "earliest", default "latest"

Where to start if no committed offset for this group.

additional_consumer_config

dict[str, Any], default {}

Extra Kafka client config (security, tuning) if you ever need it.

poll_timeout_seconds

float, default 1.0

How long consume() waits for up to worker_threads messages before returning.

Controls responsiveness when topics are idle.

controller

Optional[StreamController], default None

If provided: use it for shutdown signaling.

If None: create one and register signal handlers.

shutdown_max_wait_seconds

int, default 60

After shutdown requested:

stop polling new batches,

wait up to this long for current batch’s threads to finish,

if they don’t, exit without committing → Kafka replays that batch.

log_summary_interval

int, default 1000

Every N processed messages, log a summary (success/fail, timings, etc.).

<= 0 → disable.

log_message_details

bool, default False

If True: log per-message metadata (topic/partition/offset) – dev/debug only.

dev_mode

bool, default False

If True: never commit offsets (so you can replay data for that group_id).

Use with a dev group_id only.

3️⃣ Behavior recap (short)

Per loop:

consume(num_messages=worker_threads, timeout=poll_timeout_seconds)

Build MessageContext for each msg.

Submit to thread pool (one task per message).

Wait for all futures:

If no task reported “unhandled error” → commit batch (unless dev_mode).

If any task saw unhandled error → don’t commit, respect controller.request_stop() and exit.

On restart, Kafka replays last uncommitted batch.

All data-level error routing & retries = inside your job.
Consumer just does: threads, polling, commits, shutdown.

If you’re okay with this, this is now a very clean contract you can bake into your framework.