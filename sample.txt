import duckdb
import logging

logger = logging.getLogger(__name__)

try:
    # Create DuckDB connection (uses defaults: 80% RAM, all CPU cores)
    conn = duckdb.connect()
    
    # Build column selection with null filling
    select_columns = []
    for col in required_fields:
        if col in default_value_columns:
            # Fill nulls with '#N/A' for specified columns
            select_columns.append(f"COALESCE({col}, '#N/A') AS {col}")
        else:
            # Keep other columns as is
            select_columns.append(col)
    
    select_clause = ", ".join(select_columns)
    
    logger.info("Input file read and processed successfully using DuckDB")
    
    # Prepare output path
    output_path = f"{context['data']['output_path']}".replace(".CSV", ".parquet")
    
    # Execute the conversion in one statement with streaming processing
    query = f"""
    COPY (
        SELECT {select_clause}
        FROM read_csv(
            '{cus_balance_file_path}',
            delimiter='\t',
            header=true,
            ignore_errors=true,
            all_varchar=true
        )
    ) TO '{output_path}' 
    (FORMAT 'parquet')
    """
    
    conn.execute(query)
    
    logger.info(f"Output file written successfully as Parquet: {output_path}")
    
    # Close connection
    conn.close()
    
except Exception as e:
    logger.error(f"Failed to process or write Parquet file: {e}", exc_info=True)
    raise RuntimeError(f"Failed to process or write Parquet file: {e}")